{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- Dhruv Patel: Analysis, Software, Data curation, \n",
    "- Elthor Olivas Corral: Experimental investigation, Analysis, Writing - original draft\n",
    "- Syrine Mekni: Project administration, Conceptualization, Background research, Writing - review & editing\n",
    "- Yen-Hsianf Chiu: Methodology, Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the distribution of ChatGPT prompts shifted from primarily task-based and informational uses toward more interpersonal and intrapersonal uses (e.g., emotional reflection and relationship interpretation) between April 2023 and May 2024, as measured by topic modeling and sentiment analysis of public ChatGPT conversation data?\n",
    "\n",
    "In this study, time (April 2023â€“May 2024) is treated as the independent variable and prompt type (task-based vs interpersonal/intrapersonal) as the dependent variable, with prompt length and system-generated messages controlled for, and the analysis grounded in prior literature on humanâ€“AI interaction and the social use of conversational agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversational artificial intelligence has evolved significantly since its early origins, when systems were designed primarily for instructional or task-oriented interactions. Early conversational agents such as ELIZA were created to simulate dialogue through simple pattern matching, offering the appearance of conversation without genuine understanding and serving mainly as demonstrations of humanâ€“computer interaction rather than functional assistants.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1) For decades, most chatbots remained limited to narrow informational tasks, customer service automation, or scripted question answering. In contrast, modern large language models such as ChatGPT have rapidly gained widespread adoption and are now used by millions of people for a diverse range of purposes, extending well beyond information retrieval to include brainstorming, emotional reflection, and relationship-oriented conversations.<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) This noticeable shift from primarily task-based support toward more social and emotionally expressive interaction motivates the present study, which examines how the distribution of ChatGPT prompt types has changed between April 2023 and May 2024 using topic modeling and sentiment analysis of public conversation data.\n",
    "\n",
    "A growing body of research in humanâ€“AI interaction has examined the emotional and social dimensions of chatbot use, finding that users often engage conversational agents in ways that resemble interpersonal relationships. For example, Fitzpatrick et al. (2017) studied interactions with a fully automated mental-health chatbot and found that users frequently disclosed personal emotions and reported feeling supported, despite knowing the system was not human.<a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) Similarly, studies of social chatbots such as XiaoIce have shown that users engage in long-term, emotionally expressive conversations and may attribute relational qualities such as empathy and companionship to the system, indicating forms of parasocial or quasi-social interaction.<a name=\"cite_ref-4\"></a>[<sup>4</sup>](#cite_note-4) More recent work in Proceedings of the ACM on Humanâ€“Computer Interaction demonstrates that framing chatbot interactions as ephemeral increases users' willingness to disclose intimate information, reinforcing the idea that conversational agents are often perceived as non-judgmental confidants; however, these findings typically come from specific applications or controlled study settings rather than large-scale, open-ended usage logs.<a name=\"cite_ref-5\"></a>[<sup>5</sup>](#cite_note-5)\n",
    "\n",
    "Beyond small-scale or domain-specific studies, recent research has begun to examine how humanâ€“AI interaction evolves using large-scale conversational datasets. For example, research using the WildChat dataset analyzes over one million real-world ChatGPT conversations to characterize how users engage with conversational AI systems across a wide range of intents, demonstrating substantial diversity in prompt types and interaction styles at scale.<a name=\"cite_ref-6\"></a>[<sup>6</sup>](#cite_note-6) Related large-scale studies of language model usage similarly suggest that as models become more capable and widely deployed, user interactions diversify beyond narrowly task-oriented queries toward more exploratory, creative, and socially oriented uses. However, much of this work focuses on broad usage characterization or safety concerns rather than explicitly examining how the distribution of prompt types changes over time. In addition, some longitudinal analyses aggregate interactions across broad time spans or model versions, limiting their ability to isolate shorter-term temporal shifts in interpersonal versus task-based usage.\n",
    "\n",
    "Taken together, prior research establishes that conversational agents can elicit emotional disclosure and interpersonal engagement, but it does not directly test whether interpersonal and intrapersonal uses of ChatGPT are becoming more prevalent relative to task-based and informational uses within a recent, well-defined time window. Our project addresses this gap by using the WildChat dataset to examine whether the distribution of ChatGPT prompt types has shifted between April 2023 and May 2024 within a real-world sample of public interactions. By applying topic modeling and sentiment analysis to distinguish task-based prompts from interpersonal and intrapersonal prompts, we extend prior research on emotional disclosure and social uses of chatbots to a large-scale, temporally sensitive setting. This allows us to test whether social and self-reflective uses of ChatGPT are becoming more prevalent over time, rather than simply documenting that such uses exist.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Weizenbaum, J. (1966). ELIZAâ€”a computer program for the study of natural language communication between man and machine. *Communications of the ACM*, 9(1), 36-45. https://dl.acm.org/doi/10.1145/365153.365168\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) OpenAI. (2023). Introducing ChatGPT. https://openai.com/index/chatgpt/\n",
    "\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Fitzpatrick, K. K., Darcy, A., & Vierhile, M. (2017). Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial. *JMIR Mental Health*, 4(2), e19. https://mental.jmir.org/2017/2/e19/\n",
    "\n",
    "4. <a name=\"cite_note-4\"></a> [^](#cite_ref-4) Shum, H., He, X., & Li, D. (2018). From Eliza to XiaoIce: Challenges and opportunities with social chatbots. *Frontiers of Information Technology & Electronic Engineering*, 19(11), 1359-1384. http://link.springer.com/article/10.1631/FITEE.1800578\n",
    "\n",
    "5. <a name=\"cite_note-5\"></a> [^](#cite_ref-5) Cox, S. R., Jacobsen, R. M., & van Berkel, N. (2025). The impact of a chatbot's ephemerality-framing on self-disclosure. *Proceedings of the ACM on Human-Computer Interaction*, 9(CSCW1), Article 17. https://dl.acm.org/doi/10.1145/3719160.3736617\n",
    "\n",
    "6. <a name=\"cite_note-6\"></a> [^](#cite_ref-6) Zhao, J., Choi, Y., Smith, N. A., et al. (2024). WildChat: 1M ChatGPT interaction logs in the wild [Dataset]. Allen Institute for AI. https://wildchat.allen.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that between April 2023 and May 2024, the proportion of interpersonal and intrapersonal ChatGPT prompts will increase relative to task-based and informational prompts. This expectation is based on prior research showing that as conversational AI systems become more capable, familiar, and designed to be pleasant, approachable, and human-like, users increasingly disclose emotions, seek support, and treat them as confidants. Building on evidence that conversational AI use has diversified beyond narrowly task-oriented queries, we expect social and self-reflective interactions with ChatGPT to become more prevalent over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "### Dataset: WildChat â€“ 1M ChatGPT Interaction Logs\n",
    "\n",
    "- **Dataset Name:** WildChat  \n",
    "- **Dataset Link:** https://huggingface.co/datasets/allenai/WildChat  \n",
    "- **Number of Observations:** Approximately 1,043,000 conversations (filtered to April 2023â€“May 2024)  \n",
    "- **Number of Variables:** Includes conversation_id, timestamp, conversation (nested messages), language, model, toxicity indicators, and redacted PII flags  \n",
    "- **Key Variables for This Project:**  \n",
    "  - `conversation`: Full conversation history with speaker roles (â€œuserâ€ or â€œassistantâ€) and message content  \n",
    "  - `timestamp`: Used to filter conversations within the April 2023â€“May 2024 window  \n",
    "  - `language`: Primary language of the conversation (analysis focuses on English)  \n",
    "  - `turn`: Number of conversational turns  \n",
    "  - `user_messages`: Extracted user prompts  \n",
    "\n",
    "### Dataset Shortcomings\n",
    "\n",
    "- **Selection bias:** Only includes conversations from users who opted to share their data publicly, which may differ from the broader ChatGPT user base.  \n",
    "- **Language bias:** Predominantly English, limiting generalizability to multilingual or non-English contexts.  \n",
    "- **Lack of demographic information:** No data on age, gender, education, or other user characteristics.  \n",
    "- **Potential data quality issues:** Some conversations may include spam, experimentation, or non-serious interactions.  \n",
    "- **Missing context:** No information about usersâ€™ motivations or prior interactions with ChatGPT.\n",
    "\n",
    "This dataset is well-suited for our research question because it provides large-scale, real-world, timestamped conversations across the entire period of interest (April 2023â€“May 2024), enabling us to track how prompt types evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here is my code for uploading raw data\")\n",
    "\n",
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "# For WildChat dataset from Hugging Face, we use the datasets library instead of get_data\n",
    "# The WildChat dataset is too large to use the standard get_data approach\n",
    "# Instead, we'll download it using Hugging Face's datasets library in the next section\n",
    "\n",
    "print(\"NOTE: WildChat dataset will be loaded using Hugging Face datasets library.\")\n",
    "print(\"This dataset is large (~several GB) and will be downloaded/cached automatically.\")\n",
    "print(\"First-time download may take 10-30 minutes depending on connection speed.\")\n",
    "print(\"\\nTo download the dataset, run the cells in the 'WildChat Dataset' section below.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WildChat Dataset: Loading, Cleaning, and Wrangling\n",
    "\n",
    "The WildChat dataset contains over 1 million real-world conversations between users and ChatGPT. For our analysis, we focus on conversations from April 2023 to May 2024 to examine how prompt types have evolved over this 14-month period.\n",
    "\n",
    "**Key metrics and their meaning:**\n",
    "\n",
    "- **User prompts:** The text input from users to ChatGPT. These are our primary unit of analysis. Prompts can range from single-word queries to multi-paragraph requests. We will analyze these to categorize them as task-based (e.g., \"Write a Python function to sort a list\") versus interpersonal/intrapersonal (e.g., \"I'm feeling anxious about my job interview tomorrow, can you help me process this?\").\n",
    "\n",
    "- **Timestamp:** Unix timestamp or datetime indicating when the conversation occurred. This is critical for our temporal analysis, allowing us to track changes month-by-month from April 2023 through May 2024.\n",
    "\n",
    "- **Turn count:** Number of back-and-forth exchanges in a conversation. Higher turn counts may indicate more complex or engaging interactions. Multi-turn conversations might suggest deeper engagement, which could be more common in interpersonal exchanges.\n",
    "\n",
    "- **Prompt length (words):** Word count of user messages. Research suggests task-based prompts tend to be shorter and more directive (\"Translate this to Spanish\"), while interpersonal prompts may be longer and more narrative (\"I've been struggling with my relationship and would like to talk through some thoughts...\"). We calculate this as an approximate word count by splitting on whitespace.\n",
    "\n",
    "- **Prompt length (characters):** Character count including spaces. This provides an alternative length metric that captures very short responses and punctuation.\n",
    "\n",
    "- **Language:** ISO language code (e.g., 'en' for English, 'es' for Spanish). We focus primarily on English ('en') conversations for consistency in NLP analysis, as sentiment analysis and topic modeling tools perform best on English text.\n",
    "\n",
    "**Dataset concerns:**\n",
    "\n",
    "The dataset presents several challenges that affect our analysis:\n",
    "\n",
    "1. **Size and computational constraints:** The dataset is extremely large (several GB), containing over 1 million conversations. To make processing manageable, we work with filtered subsets based on our time period (April 2023 - May 2024) and language (English). Even after filtering, the dataset may contain hundreds of thousands of prompts, requiring efficient data structures and potentially sampling for some analyses.\n",
    "\n",
    "2. **Temporal data quality:** Not all conversations have clean timestamp data. Some entries may have null or malformed timestamps that need to be excluded. Additionally, the temporal distribution is unevenâ€”some months have significantly more data than others, which could reflect changes in data collection methods, ChatGPT popularity, or user behavior patterns.\n",
    "\n",
    "3. **Language diversity and code-switching:** While we filter for English conversations, some conversations may contain multiple languages or code-switching (alternating between languages). Technical discussions may include code snippets in programming languages, which we need to handle appropriately in our text analysis.\n",
    "\n",
    "4. **Privacy and content concerns:** The WildChat creators have applied PII (personally identifiable information) redaction, but some personal content may remain. Privacy redaction may affect our ability to analyze certain types of personal or emotional contentâ€”ironically, the most interpersonal conversations may have had the most redaction. We also need to be mindful that some conversations may contain sensitive topics that require ethical handling.\n",
    "\n",
    "5. **Conversation structure variability:** The dataset includes both single-turn conversations (one user prompt, one assistant response) and multi-turn conversations (extended back-and-forth). We need to decide whether to analyze each user prompt independently or consider conversation-level patterns. For our analysis, we treat each user prompt as an independent observation while also tracking which conversation it belongs to.\n",
    "\n",
    "6. **Ground truth absence:** We have no verified labels for what constitutes \"task-based\" versus \"interpersonal/intrapersonal\" prompts. We will need to develop and validate our categorization approach through topic modeling and potentially manual review of samples.\n",
    "\n",
    "7. **Selection bias:** Users who share their conversations publicly may differ systematically from those who don't. People sharing emotional or personal conversations might be more open about mental health, while those with sensitive work queries might not share at all. This could affect our ability to detect genuine temporal trends versus shifts in who is sharing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"\\nNote: If you get import errors, install required packages:\")\n",
    "print(\"pip install datasets pandas numpy matplotlib seaborn tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WildChat dataset from Hugging Face\n",
    "# Note: This will download the dataset if it's not already cached\n",
    "# The full dataset is large (~several GB), so this may take some time on first run\n",
    "\n",
    "print(\"Loading WildChat dataset from Hugging Face...\")\n",
    "print(\"Note: First-time download may take 10-30 minutes depending on connection speed.\")\n",
    "print(\"Subsequent runs will use the cached version.\\n\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"allenai/WildChat\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total conversations in full dataset: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier manipulation\n",
    "print(\"Converting to pandas DataFrame...\")\n",
    "df_raw = dataset.to_pandas()\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df_raw.shape}\")\n",
    "print(f\"\\nColumns available:\")\n",
    "for col in df_raw.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "\n",
    "print(\"\\nSaving raw dataset to disk...\")\n",
    "\n",
    "raw_path = \"data/00-raw/wildchat_raw.pkl\"\n",
    "df_raw.to_pickle(raw_path)\n",
    "\n",
    "print(f\"Raw dataset saved to: {raw_path}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"First few rows:\")\n",
    "print(\"=\"*60)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of the data\n",
    "print(\"Dataset Info:\")\n",
    "print(df_raw.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample conversation structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_conv = df_raw.iloc[0]['conversation']\n",
    "\n",
    "# If it's a numpy array, convert to list\n",
    "if isinstance(sample_conv, np.ndarray):\n",
    "    sample_conv = sample_conv.tolist()\n",
    "\n",
    "# If it's a string, parse it\n",
    "if isinstance(sample_conv, str):\n",
    "    sample_conv = json.loads(sample_conv)\n",
    "\n",
    "# Now sample_conv is guaranteed to be a list\n",
    "print(\"\\nFirst 2 messages from a sample conversation:\")\n",
    "print(json.dumps(sample_conv[:2], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of the data\n",
    "print(\"Dataset Info:\")\n",
    "print(df_raw.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample conversation structure:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_conv = df_raw.iloc[0]['conversation']\n",
    "\n",
    "# If it's a numpy array, convert to list\n",
    "if isinstance(sample_conv, np.ndarray):\n",
    "    sample_conv = sample_conv.tolist()\n",
    "\n",
    "# If it's a string, parse it\n",
    "if isinstance(sample_conv, str):\n",
    "    sample_conv = json.loads(sample_conv)\n",
    "\n",
    "# Now sample_conv is guaranteed to be a list\n",
    "print(\"\\nFirst 2 messages from a sample conversation:\")\n",
    "print(json.dumps(sample_conv[:2], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user prompts from conversations\n",
    "print(\"Extracting user prompts from conversations...\\n\")\n",
    "\n",
    "def extract_user_prompts(conversation):\n",
    "    \"\"\"\n",
    "    Extract all user messages from a conversation.\n",
    "    Handles lists, numpy arrays, dict-wrapped messages, and JSON strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(conversation, np.ndarray):\n",
    "        conversation = conversation.tolist()\n",
    "\n",
    "    # If it's a dict with a 'messages' key, unwrap it\n",
    "    if isinstance(conversation, dict) and 'messages' in conversation:\n",
    "        conversation = conversation['messages']\n",
    "\n",
    "    # If it's a JSON string, parse it\n",
    "    if isinstance(conversation, str):\n",
    "        try:\n",
    "            conversation = json.loads(conversation)\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "    # If it's still not a list, we can't extract messages\n",
    "    if not isinstance(conversation, list):\n",
    "        return []\n",
    "\n",
    "    # Extract user messages\n",
    "    user_prompts = []\n",
    "    for message in conversation:\n",
    "        if isinstance(message, dict) and message.get('role') == 'user':\n",
    "            content = message.get('content', '')\n",
    "            if isinstance(content, str) and content.strip():\n",
    "                user_prompts.append(content.strip())\n",
    "\n",
    "    return user_prompts\n",
    "\n",
    "# Apply extraction with progress bar\n",
    "tqdm.pandas(desc=\"Extracting prompts\")\n",
    "df_filtered['user_prompts'] = df_filtered['conversation'].progress_apply(extract_user_prompts)\n",
    "\n",
    "# Count number of user prompts per conversation\n",
    "df_filtered['num_user_prompts'] = df_filtered['user_prompts'].apply(len)\n",
    "\n",
    "print(f\"\\nExtraction complete!\")\n",
    "print(f\"Conversations with at least one user prompt: {(df_filtered['num_user_prompts'] > 0).sum():,}\")\n",
    "print(f\"Total user prompts extracted: {df_filtered['num_user_prompts'].sum():,}\")\n",
    "print(f\"Average prompts per conversation: {df_filtered['num_user_prompts'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt-level dataset (one row per prompt)\n",
    "print(\"Creating prompt-level dataset...\\n\")\n",
    "\n",
    "# Explode the user_prompts list so each prompt gets its own row\n",
    "df_prompts = df_filtered.explode('user_prompts').reset_index(drop=True)\n",
    "\n",
    "# Remove rows with no prompt\n",
    "df_prompts = df_prompts[df_prompts['user_prompts'].notna() & (df_prompts['user_prompts'] != '')].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "df_prompts.rename(columns={'user_prompts': 'prompt'}, inplace=True)\n",
    "\n",
    "print(f\"Prompt-level dataset shape: {df_prompts.shape}\")\n",
    "print(f\"Total prompts: {len(df_prompts):,}\")\n",
    "\n",
    "print(\"\\nSample prompts:\")\n",
    "print(\"=\"*60)\n",
    "for i, prompt in enumerate(df_prompts['prompt'].head(3), 1):\n",
    "    print(f\"{i}. {prompt[:100]}...\" if len(prompt) > 100 else f\"{i}. {prompt}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prompt characteristics\n",
    "print(\"Calculating prompt characteristics...\\n\")\n",
    "\n",
    "# Prompt length in characters\n",
    "df_prompts['prompt_length_chars'] = df_prompts['prompt'].str.len()\n",
    "\n",
    "# Prompt length in words (approximate)\n",
    "df_prompts['prompt_length_words'] = df_prompts['prompt'].str.split().str.len()\n",
    "\n",
    "# Extract month and year for temporal analysis\n",
    "df_prompts['year_month'] = df_prompts['datetime'].dt.to_period('M')\n",
    "df_prompts['year'] = df_prompts['datetime'].dt.year\n",
    "df_prompts['month'] = df_prompts['datetime'].dt.month\n",
    "\n",
    "print(\"Prompt characteristics added:\")\n",
    "print(\"  âœ“ prompt_length_chars\")\n",
    "print(\"  âœ“ prompt_length_words\")\n",
    "print(\"  âœ“ year_month\")\n",
    "print(\"  âœ“ year\")\n",
    "print(\"  âœ“ month\")\n",
    "\n",
    "print(\"\\nPrompt length summary:\")\n",
    "print(df_prompts[['prompt_length_chars', 'prompt_length_words']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(\"Checking for missing data...\\n\")\n",
    "print(\"Missing values by column:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': df_prompts.isnull().sum(),\n",
    "    'Percentage': (df_prompts.isnull().sum() / len(df_prompts) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data)\n",
    "    \n",
    "    # Visualize missingness\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(missing_data.index, missing_data['Missing Count'])\n",
    "    plt.xlabel('Number of Missing Values')\n",
    "    plt.title('Missing Data by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ“ No missing values found in any columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to English language conversations (primary focus)\n",
    "print(\"Filtering to English language conversations...\\n\")\n",
    "\n",
    "if 'language' in df_prompts.columns:\n",
    "    print(\"Language distribution (top 10):\")\n",
    "    print(\"=\"*60)\n",
    "    lang_dist = df_prompts['language'].value_counts().head(10)\n",
    "    for lang, count in lang_dist.items():\n",
    "        pct = count / len(df_prompts) * 100\n",
    "        print(f\"  {lang}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Keep only English\n",
    "    df_prompts_en = df_prompts[df_prompts['language'] == 'English'].copy()\n",
    "    \n",
    "    print(f\"\\nPrompts after English filter: {len(df_prompts_en):,}\")\n",
    "    print(f\"Percentage retained: {len(df_prompts_en)/len(df_prompts)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"Warning: No language column found. Proceeding with all conversations.\")\n",
    "    df_prompts_en = df_prompts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle outliers in prompt length\n",
    "print(\"Identifying outliers in prompt length...\\n\")\n",
    "\n",
    "# Summary statistics for prompt length\n",
    "print(\"Prompt length statistics (words):\")\n",
    "print(\"=\"*60)\n",
    "print(df_prompts_en['prompt_length_words'].describe())\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_prompts_en['prompt_length_words'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Prompt Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Prompt Lengths')\n",
    "axes[0].set_xlim(0, 200)  # Focus on reasonable range\n",
    "axes[0].axvline(df_prompts_en['prompt_length_words'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_prompts_en['prompt_length_words'], vert=True)\n",
    "axes[1].set_ylabel('Prompt Length (words)')\n",
    "axes[1].set_title('Prompt Length Box Plot')\n",
    "axes[1].set_xticklabels(['Prompts'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify extreme outliers (using IQR method)\n",
    "Q1 = df_prompts_en['prompt_length_words'].quantile(0.25)\n",
    "Q3 = df_prompts_en['prompt_length_words'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR  # Using 3*IQR for extreme outliers\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "outliers = df_prompts_en[\n",
    "    (df_prompts_en['prompt_length_words'] < lower_bound) | \n",
    "    (df_prompts_en['prompt_length_words'] > upper_bound)\n",
    "]\n",
    "\n",
    "print(f\"\\nExtreme outliers detected: {len(outliers):,} ({len(outliers)/len(df_prompts_en)*100:.2f}%)\")\n",
    "print(f\"Lower bound: {lower_bound:.1f} words\")\n",
    "print(f\"Upper bound: {upper_bound:.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "print(\"Cleaning data...\\n\")\n",
    "\n",
    "initial_count = len(df_prompts_en)\n",
    "\n",
    "# Remove extremely short prompts (likely noise, testing, or errors)\n",
    "# We'll keep prompts with at least 2 words\n",
    "df_clean = df_prompts_en[df_prompts_en['prompt_length_words'] >= 2].copy()\n",
    "removed_short = initial_count - len(df_clean)\n",
    "print(f\"âœ“ Removed {removed_short:,} prompts with < 2 words\")\n",
    "\n",
    "# Remove extremely long prompts (likely copy-paste of documents, spam, or errors)\n",
    "# We'll cap at 99th percentile\n",
    "length_threshold = df_clean['prompt_length_words'].quantile(0.99)\n",
    "before_long = len(df_clean)\n",
    "df_clean = df_clean[df_clean['prompt_length_words'] <= length_threshold].copy()\n",
    "removed_long = before_long - len(df_clean)\n",
    "print(f\"âœ“ Removed {removed_long:,} prompts longer than {length_threshold:.0f} words (99th percentile)\")\n",
    "\n",
    "# Remove any rows with missing critical fields\n",
    "critical_columns = ['prompt', 'datetime', 'year_month']\n",
    "before_missing = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=critical_columns)\n",
    "removed_missing = before_missing - len(df_clean)\n",
    "print(f\"âœ“ Removed {removed_missing:,} rows with missing critical data\")\n",
    "\n",
    "# Remove duplicate prompts from the same conversation ID on the same day\n",
    "# (Keep first occurrence)\n",
    "if 'conversation_id' in df_clean.columns:\n",
    "    before_dedup = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['conversation_id', 'prompt', 'datetime'], keep='first')\n",
    "    removed_dupes = before_dedup - len(df_clean)\n",
    "    print(f\"âœ“ Removed {removed_dupes:,} duplicate prompts\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"CLEANING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Initial prompts: {initial_count:,}\")\n",
    "print(f\"Final cleaned prompts: {len(df_clean):,}\")\n",
    "print(f\"Total removed: {initial_count - len(df_clean):,}\")\n",
    "print(f\"Retention rate: {len(df_clean)/initial_count*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data cleanliness with comprehensive visualizations\n",
    "print(\"Verifying data cleanliness...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Temporal distribution\n",
    "temporal_dist = df_clean.groupby('year_month').size()\n",
    "temporal_dist.plot(kind='bar', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Number of Prompts')\n",
    "axes[0, 0].set_title('Prompts per Month (Apr 2023 - May 2024)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Prompt length distribution (cleaned)\n",
    "axes[0, 1].hist(df_clean['prompt_length_words'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].axvline(df_clean['prompt_length_words'].mean(), color='red', linestyle='--', \n",
    "                   label=f\"Mean: {df_clean['prompt_length_words'].mean():.1f}\")\n",
    "axes[0, 1].axvline(df_clean['prompt_length_words'].median(), color='orange', linestyle='--', \n",
    "                   label=f\"Median: {df_clean['prompt_length_words'].median():.1f}\")\n",
    "axes[0, 1].set_xlabel('Prompt Length (words)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Cleaned Prompt Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Prompt length over time\n",
    "monthly_length = df_clean.groupby('year_month')['prompt_length_words'].agg(['mean', 'std'])\n",
    "monthly_length['mean'].plot(ax=axes[1, 0], marker='o', color='coral', linewidth=2, markersize=6)\n",
    "axes[1, 0].fill_between(monthly_length.index.astype(str), \n",
    "                        monthly_length['mean'] - monthly_length['std'],\n",
    "                        monthly_length['mean'] + monthly_length['std'],\n",
    "                        alpha=0.2, color='coral')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Prompt Length (words)')\n",
    "axes[1, 0].set_title('Average Prompt Length Over Time (Â±1 SD)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Data completeness check\n",
    "completeness = pd.DataFrame({\n",
    "    'Column': critical_columns,\n",
    "    'Completeness %': [100 - (df_clean[col].isna().sum() / len(df_clean) * 100) for col in critical_columns]\n",
    "})\n",
    "colors = ['green' if x == 100 else 'orange' for x in completeness['Completeness %']]\n",
    "axes[1, 1].barh(completeness['Column'], completeness['Completeness %'], color=colors, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Completeness (%)')\n",
    "axes[1, 1].set_title('Data Completeness for Critical Columns', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlim(95, 100)\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(completeness['Completeness %']):\n",
    "    axes[1, 1].text(v - 0.5, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/data_cleanliness_check.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Data cleanliness visualizations saved to: data/02-processed/data_cleanliness_check.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL CLEANED DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Size:\")\n",
    "print(f\"  Total prompts: {len(df_clean):,}\")\n",
    "print(f\"  Unique conversations: {df_clean['conversation_id'].nunique() if 'conversation_id' in df_clean.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nðŸ“… Temporal Coverage:\")\n",
    "print(f\"  Start date: {df_clean['datetime'].min()}\")\n",
    "print(f\"  End date: {df_clean['datetime'].max()}\")\n",
    "print(f\"  Duration: {(df_clean['datetime'].max() - df_clean['datetime'].min()).days} days\")\n",
    "print(f\"  Months covered: {df_clean['year_month'].nunique()}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Prompt Length Statistics (words):\")\n",
    "length_stats = df_clean['prompt_length_words'].describe()\n",
    "for stat in ['mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
    "    print(f\"  {stat:>6s}: {length_stats[stat]:>8.1f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Monthly Distribution:\")\n",
    "monthly_counts = df_clean.groupby('year_month').size()\n",
    "print(f\"  Average prompts per month: {monthly_counts.mean():.0f}\")\n",
    "print(f\"  Min prompts in a month: {monthly_counts.min():,}\")\n",
    "print(f\"  Max prompts in a month: {monthly_counts.max():,}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Data Types:\")\n",
    "key_cols = ['prompt', 'datetime', 'prompt_length_words', 'year_month']\n",
    "for col in key_cols:\n",
    "    print(f\"  {col}: {df_clean[col].dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned dataset\n",
    "print(\"Saving cleaned dataset...\\n\")\n",
    "\n",
    "# Select relevant columns for analysis\n",
    "columns_to_save = [\n",
    "    'conversation_id',\n",
    "    'prompt',\n",
    "    'datetime',\n",
    "    'year_month',\n",
    "    'year',\n",
    "    'month',\n",
    "    'prompt_length_chars',\n",
    "    'prompt_length_words',\n",
    "    'language',\n",
    "    'turn',\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "columns_to_save = [col for col in columns_to_save if col in df_clean.columns]\n",
    "\n",
    "df_final = df_clean[columns_to_save].copy()\n",
    "\n",
    "print(f\"Selected {len(columns_to_save)} columns for final dataset:\")\n",
    "for col in columns_to_save:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Save in multiple formats for flexibility\n",
    "print(\"\\nSaving files...\")\n",
    "\n",
    "# Pickle format (preserves data types)\n",
    "df_final.to_pickle('data/02-processed/wildchat_prompts_cleaned.pkl')\n",
    "print(\"  âœ“ Pickle: data/02-processed/wildchat_prompts_cleaned.pkl\")\n",
    "\n",
    "# CSV format (human-readable)\n",
    "df_final.to_csv('data/02-processed/wildchat_prompts_cleaned.csv', index=False)\n",
    "print(\"  âœ“ CSV: data/02-processed/wildchat_prompts_cleaned.csv\")\n",
    "\n",
    "# Save a sample for quick inspection\n",
    "sample_size = min(1000, len(df_final))\n",
    "df_final.sample(sample_size).to_csv('data/02-processed/wildchat_prompts_sample.csv', index=False)\n",
    "print(f\"  âœ“ Sample ({sample_size:,} rows): data/02-processed/wildchat_prompts_sample.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATA WRANGLING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Topic modeling to categorize prompts\")\n",
    "print(\"  2. Sentiment analysis\")\n",
    "print(\"  3. Temporal analysis of prompt type distribution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a team, we recognize that data analysis is not neutral. Even descriptive analytics can influence decisions, shape narratives, and create downstream consequences. Because of this, we approach our work with careful attention to ethical responsibility, fairness, and unintended impacts.\n",
    "\n",
    "First, we will ensure that all data used in this project is legally obtained and ethically appropriate for academic analysis. If working with conversation data such as ChatGPT interactions, we acknowledge the heightened sensitivity of such information. Even if data is anonymized, conversational data may contain personal, emotional, or behavioral signals that could be misused. We will avoid exposing identifying details, refrain from attempting re-identification, and treat conversational data with additional care due to its inherently personal nature.\n",
    "\n",
    "Second, we explicitly consider unintended uses of our findings. Analytical results can be repurposed beyond their original intent. For example, insights derived from conversational data could potentially be used for surveillance, profiling, behavioral manipulation, or targeted persuasion. Even aggregate patterns may enable systems that categorize or predict user behavior in ways that limit autonomy. We will clearly state the scope of our analysis and discourage interpretations that extend beyond the evidence.\n",
    "\n",
    "Third, we are attentive to fairness across groups. Data often reflects existing social inequalities, historical bias, and uneven representation. Even descriptive statistics can reinforce stereotypes if presented without context. We will examine whether certain groups are underrepresented, overrepresented, or systematically portrayed differently in the dataset. Where disparities appear, we will avoid causal claims without evidence and will acknowledge structural explanations rather than attributing differences to inherent characteristics.\n",
    "\n",
    "Additionally, we recognize that descriptive analytics can still contribute to downstream bias mitigation or harm. For instance, identifying patterns may influence future model training, policy decisions, or product design choices. If our results highlight disparities, those findings could be used either to improve fairness or to justify exclusionary practices. Because of this, we will frame conclusions carefully, document limitations, and avoid deterministic interpretations.\n",
    "\n",
    "Finally, we commit to transparency and academic integrity. All preprocessing steps, assumptions, and limitations will be clearly documented. External tools, including AI systems, will be used in accordance with course guidelines and properly acknowledged.\n",
    "\n",
    "Our goal is not simply to generate accurate results, but to ensure that the knowledge produced is responsible, context-aware, and mindful of its broader social impact."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maintain accountability and fairness within the group, we have established the following expectations:\n",
    "\n",
    "Equal Contribution\n",
    "Each team member is expected to actively contribute to coding, analysis, writing, and discussion. Work will be distributed clearly to avoid imbalance.\n",
    "\n",
    "Communication\n",
    "We will communicate regularly through agreed-upon platforms and respond within a reasonable timeframe (e.g., 24 hours). If someone is unable to complete a task, they must notify the team early.\n",
    "\n",
    "Deadlines\n",
    "Internal deadlines will be set before the official due date to allow time for review and revisions. All members are responsible for meeting agreed deadlines.\n",
    "\n",
    "Respect and Professionalism\n",
    "Team discussions will remain respectful and constructive. Disagreements will be handled through open discussion and evidence-based reasoning.\n",
    "\n",
    "Accountability\n",
    "Each member will review the final submission to ensure they understand and support the work. No one will submit work they did not review.\n",
    "\n",
    "Transparency\n",
    "All code will be shared through version control (e.g., Git) so contributions are visible and documented.\n",
    "\n",
    "By setting clear expectations, we aim to create a collaborative, organized, and fair working environment that supports both high-quality work and positive team dynamics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
